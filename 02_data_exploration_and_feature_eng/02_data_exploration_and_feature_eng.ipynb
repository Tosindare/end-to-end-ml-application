{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data exploration and preparation</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this and the following notebooks we will demonstrate how you can build your ML Pipeline leveraging Spark Feature Transformers and SageMaker XGBoost algorithm & after the model is trained, deploy the Pipeline (Feature Transformer and XGBoost) as an Inference Pipeline behind a single Endpoint for real-time inference and for batch inferences using Amazon SageMaker Batch Transform.\n",
    "\n",
    "In particular, in this notebook we will tackle the first steps related to data exploration and preparation. We will use [Amazon Athena](https://aws.amazon.com/athena/) to query our dataset and have a first insight about data quality and available features, and [AWS Glue](https://aws.amazon.com/glue/) to create a Data Catalog and run serverless Spark jobs.\n",
    "\n",
    "<span style=\"color: red\">**To get started, in the cell below please replace your initials in the bucket_name variable, in order to match the bucket name you've created in the previous steps.**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(region)\n",
    "print(role)\n",
    " \n",
    "# replace [your-initials] according to the bucket name you have defined.\n",
    "bucket_name = 'endtoendml-workshop-[your-initials]'\n",
    "\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now copy to our bucket the dataset used for this use case. We will use the `windturbine_raw_data.csv` made available for this workshop in the `gianpo-public` public S3 bucket. In this Notebook, we will download from that bucket and upload to your bucket so that AWS Glue can access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "copy_source = {\n",
    "    'Bucket': 'gianpo-public',\n",
    "    'Key': 'windturbine_raw_data.csv'\n",
    "}\n",
    "\n",
    "file_name = 'windturbine_raw_data.csv'\n",
    "file_key = 'data/{0}'.format(file_name)\n",
    "s3.Bucket(bucket_name).object_versions.delete()\n",
    "s3.Bucket(bucket_name).copy(copy_source, file_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need now is to infer a schema for our dataset. Thanks to its [integration with AWS Glue](https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html), we will later use Amazon Athena to run SQL queries against our data stored in S3 without the need to import them into a relational database. To do so, Amazon Athena uses the AWS Glue Data Catalog as a central location to store and retrieve table metadata throughout an AWS account. The Athena execution engine, indeed, requires table metadata that instructs it where to read data, how to read it, and other information necessary to process the data.\n",
    "\n",
    "To organize our Glue Data Catalog we create a new database named `endtoendml-db`. To do so, we create a Glue client via Boto and invoke the `create_database` method.\n",
    "\n",
    "However, first we want to make sure these objects to not exist yet to avoid any error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client('glue')\n",
    "\n",
    "# Trying to remove any existing database/crawler with the same name.\n",
    "\n",
    "crawler_found = True\n",
    "try:\n",
    "    glue_client.get_crawler(Name = 'endtoendml-crawler')\n",
    "except glue_client.exceptions.EntityNotFoundException:\n",
    "    crawler_found = False\n",
    "\n",
    "db_found = True\n",
    "try:\n",
    "    glue_client.get_database(Name = 'endtoendml-db')\n",
    "except glue_client.exceptions.EntityNotFoundException:\n",
    "    db_found = False\n",
    "\n",
    "if crawler_found:\n",
    "    glue_client.delete_crawler(Name = 'endtoendml-crawler')\n",
    "if db_found:\n",
    "     glue_client.delete_database(Name = 'endtoendml-db')\n",
    "\n",
    "print(\"Cleanup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue_client.create_database(DatabaseInput={'Name': 'endtoendml-db'})\n",
    "response = glue_client.get_database(Name='endtoendml-db')\n",
    "response\n",
    "assert response['Database']['Name'] == 'endtoendml-db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a Glue Crawler that we point to the S3 path where the dataset resides, and the crawler creates table definitions in the Data Catalog.\n",
    "To grant the correct set of access permission to the crawler, we use one of the roles created before (`GlueServiceRole-endtoendml`) whose policy grants AWS Glue access to data stored in your S3 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue_client.create_crawler(\n",
    "    Name='endtoendml-crawler',\n",
    "    Role='service-role/GlueServiceRole-endtoendml', \n",
    "    DatabaseName='endtoendml-db',\n",
    "    Targets={'S3Targets': [{'Path': '{0}/data/'.format(bucket_name)}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to run the crawler with the `start_crawler` API and to monitor its status upon completion through the `get_crawler_metrics` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.start_crawler(Name='endtoendml-crawler')\n",
    "\n",
    "while glue_client.get_crawler_metrics(CrawlerNameList=['endtoendml-crawler'])['CrawlerMetricsList'][0]['TablesCreated'] == 0:\n",
    "    print('RUNNING')\n",
    "    time.sleep(15)\n",
    "    \n",
    "assert glue_client.get_crawler_metrics(CrawlerNameList=['endtoendml-crawler'])['CrawlerMetricsList'][0]['TablesCreated'] == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the crawler has finished its job, we can retrieve the Table definition for the newly created table.\n",
    "As you can see, the crawler has been able to correctly identify 12 fields, infer a type for each column and assign a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = glue_client.get_table(DatabaseName='endtoendml-db', Name='data')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our knowledge of the dataset, we can assign more specific names to columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table['Table']['StorageDescriptor']['Columns'] = [{'Name': 'turbine_id', 'Type': 'string'},\n",
    "                                                  {'Name': 'turbine_type', 'Type': 'string'},\n",
    "                                                  {'Name': 'wind_speed', 'Type': 'double'},\n",
    "                                                  {'Name': 'rpm_blade', 'Type': 'double'},\n",
    "                                                  {'Name': 'oil_temperature', 'Type': 'double'},\n",
    "                                                  {'Name': 'oil_level', 'Type': 'double'},\n",
    "                                                  {'Name': 'temperature', 'Type': 'double'},\n",
    "                                                  {'Name': 'humidity', 'Type': 'double'},\n",
    "                                                  {'Name': 'vibrations_frequency', 'Type': 'double'},\n",
    "                                                  {'Name': 'pressure', 'Type': 'double'},\n",
    "                                                  {'Name': 'wind_direction', 'Type': 'string'},\n",
    "                                                  {'Name': 'breakdown', 'Type': 'string'}]\n",
    "updated_table = table['Table']\n",
    "updated_table.pop('DatabaseName', None)\n",
    "updated_table.pop('CreateTime', None)\n",
    "updated_table.pop('UpdateTime', None)\n",
    "updated_table.pop('CreatedBy', None)\n",
    "updated_table.pop('IsRegisteredWithLakeFormation', None)\n",
    "\n",
    "glue_client.update_table(\n",
    "    DatabaseName='endtoendml-db',\n",
    "    TableInput=updated_table\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyathena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyathena\n",
    "from pyathena import connect\n",
    "from pyathena.pandas_cursor import PandasCursor\n",
    "import pandas as pd\n",
    "\n",
    "conn = connect(s3_staging_dir='s3://{0}/staging/'.format(bucket_name), \n",
    "               region_name=region, cursor_class=PandasCursor)\n",
    "\n",
    "df = pd.read_sql('SELECT * FROM \"endtoendml-db\".data limit 8;', conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another SQL query to count how many records we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('SELECT COUNT(*) FROM \"endtoendml-db\".data;', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to see what are possible values for the field \"alarm\" and how frequently they occur over the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('SELECT breakdown, (COUNT(breakdown) * 100.0 / (SELECT COUNT(*) FROM \"endtoendml-db\".data)) \\\n",
    "            AS percent FROM \"endtoendml-db\".data GROUP BY breakdown;', conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('SELECT DISTINCT(turbine_type) FROM \"endtoendml-db\".data', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('SELECT COUNT(*) FROM \"endtoendml-db\".data WHERE oil_temperature IS NULL GROUP BY oil_temperature', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see if there is a correlation between temperature and humidity. To do so we run a SQL query to select only these two columns and populate a Pandas dataframe that we will use for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_hum_df = pd.read_sql('SELECT temperature, humidity FROM \"endtoendml-db\".data', conn)\n",
    "temp_hum_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(temp_hum_df.temperature, temp_hum_df.humidity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(temp_hum_df.humidity, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_rpm_df = pd.read_sql('SELECT wind_speed, rpm_blade FROM \"endtoendml-db\".data', conn)\n",
    "plt.scatter(wind_rpm_df.wind_speed, wind_rpm_df.rpm_blade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can go to Amazon Athena console and check for query duration under History tab: usually queries are executed in a few seconds, then it takes a while for pandas to load results into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_rpm_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we select our entire dataset and populate a dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('SELECT * FROM \"endtoendml-db\".data;', conn)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice that col4float has some missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=['object', 'int64', 'float64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/python/python.zip\n",
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/0.9.6/jar/mleap_spark_assembly.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.Bucket(bucket_name).upload_file('python.zip', 'dependencies/python/python.zip')\n",
    "s3.Bucket(bucket_name).upload_file('mleap_spark_assembly.jar', 'dependencies/jar/mleap_spark_assembly.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize endtoendml_etl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.Bucket(bucket_name).upload_file('endtoendml_etl.py', 'code/endtoendml_etl.py')\n",
    "\n",
    "ETLJob = glue_client.create_job(Name='endtoendml-job', \n",
    "                                Role='GlueServiceRole-endtoendml',\n",
    "                                Command={\n",
    "                                    'Name': 'glueetl',\n",
    "                                    'ScriptLocation': 's3://{0}/code/endtoendml_etl.py'.format(bucket_name)\n",
    "                                },\n",
    "                               DefaultArguments={\n",
    "                                   '--job-language': 'python',\n",
    "                                   '--extra-jars' : 's3://{0}/dependencies/jar/mleap_spark_assembly.jar'.format(bucket_name),\n",
    "                                   '--extra-py-files': 's3://{0}/dependencies/python/python.zip'.format(bucket_name)\n",
    "                               })\n",
    "glue_job_name = ETLJob['Name']\n",
    "print(glue_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JobRun = glue_client.start_job_run(JobName=glue_job_name, \n",
    "                                  Arguments = {'--S3_BUCKET': bucket_name})\n",
    "print(JobRun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = glue_client.get_job_run(JobName=ETLJob['Name'], RunId=JobRun['JobRunId'])\n",
    "while status['JobRun']['JobRunState'] not in ('FAILED', 'SUCCEEDED', 'STOPPED'):\n",
    "    print('Job status: ' + status['JobRun']['JobRunState'])\n",
    "    time.sleep(30)\n",
    "    status = glue_client.get_job_run(JobName=ETLJob['Name'], RunId=JobRun['JobRunId'])\n",
    "\n",
    "print(status['JobRun']['JobRunState'])\n",
    "    \n",
    "#This will take around 15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the job is completed, you can move to the next notebook in the **03_train_model** folder to start model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
